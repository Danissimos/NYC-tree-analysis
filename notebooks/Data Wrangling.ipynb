{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59ebde3-70ca-46e1-aeb5-b7ebb4b2094c",
   "metadata": {},
   "source": [
    "# **Imports and defined functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b415bd-efc2-4c0c-a176-2ba04d647c13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb89fbe9-998a-42ab-8f02-03fbd140ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Libraries for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e3a89-5183-46ba-a5e2-290f1d301302",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Defined functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4253ac4f-5998-4bfe-824e-49e0cf33214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_manhattan(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    delta_lat = np.abs(np.radians(lat1) - np.radians(lat2))\n",
    "    delta_lon = np.abs(np.radians(lon1) - np.radians(lon2))\n",
    "    R = 6731 # Earth radius, in km\n",
    "\n",
    "    a = np.sin(delta_lat / 2) ** 2\n",
    "    b = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    latitude_distance = R * b\n",
    "\n",
    "    c = np.sin(delta_lon / 2) ** 2\n",
    "    d = 2 * np.arctan2(np.sqrt(c), np.sqrt(1 - c))\n",
    "    longitude_distance = R * d\n",
    "\n",
    "    d = np.abs(latitude_distance) + np.abs(longitude_distance) # in km\n",
    "\n",
    "    return d # in km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b83fb9-7670-430d-80ab-4262b25b4858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_and_pca(data, variable, ngram_range, token_pattern, n_components):\n",
    "    \"\"\"\n",
    "        This function gets a categorial/text variable, vectorizes it, then performce PCA dimencial reduction.\n",
    "\n",
    "        Parameters:\n",
    "                data(pd.DataFrame): dataframe with data\n",
    "                variable(str): name of a column to vectorize from\n",
    "                ngram_range(tuple): tuple of values to make n_grams from. Examples: (1, 1), (2, 2), (3, 3)\n",
    "                token_pattern(str): token pattern for CountVectorizer, used where specific splitter needed\n",
    "                n_components(int): number of PCA-components\n",
    "\n",
    "            Returns:\n",
    "                Returns transformed and normed data\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    vec = CountVectorizer(ngram_range = ngram_range, token_pattern = token_pattern) # строим BoW для слов\n",
    "    bow = vec.fit_transform(data[variable])\n",
    "    \n",
    "    pca = PCA(n_components = n_components)\n",
    "    pca.fit(bow)\n",
    "\n",
    "    col_names = [variable + '_pca_component_' + str(i) for i in range(1, n_components + 1)]\n",
    "    data_transformed = pca.fit_transform(bow)\n",
    "    data_transformed = pd.DataFrame(data_transformed, columns = col_names)\n",
    "    \n",
    "    scaler = MinMaxScaler().fit(data_transformed)\n",
    "    data_transformed_normed = pd.DataFrame(scaler.fit_transform(data_transformed.values), columns = data_transformed.columns, index = data_transformed.index)\n",
    "\n",
    "    return vec, pca, scaler, data_transformed_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca79a60-dcca-4709-8efb-274b6ea8d880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrangling(df, lat_lon_poor_health):\n",
    "    \"\"\"\n",
    "        Data wrangling of given df\n",
    "    \"\"\"\n",
    "\n",
    "    list_of_results_haversine_manhattan = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        data_merged = pd.merge(df.iloc[i:i+1], lat_lon_poor_health, how = 'cross')\n",
    "        \n",
    "        list_of_distances = haversine_manhattan(data_merged['latitude'], data_merged['longitude'], data_merged['latitude_x'], data_merged['longitude_x'])\n",
    "        list_of_distances.replace(0, np.nan, inplace = True)\n",
    "        list_of_results_haversine_manhattan.append(list_of_distances.min())\n",
    "    \n",
    "    df['haversine_manhattan_distance_to_poor_tree'] = list_of_results_haversine_manhattan\n",
    "    \n",
    "    df['tree_dbh_wrangled'] = np.where(df['tree_dbh'] > 35, 35, df['tree_dbh'])\n",
    "    \n",
    "    df['problems_count'] = df['problems'].str.split(',').str.len()\n",
    "    df['problems_count'] = np.where(df['problems'] == 'None', 0, df['problems_count'])\n",
    "    df['problems_count'] = np.where(df['problems_count'] > 4, 4, df['problems_count'])\n",
    "    df['has_problem'] = np.where(df['problems_count'] > 0, 1, 0)\n",
    "    \n",
    "    list_of_problems = ['Stones', 'BranchLights', 'TrunkOther', 'BranchOther',\n",
    "                    'RootOther', 'WiresRope', 'MetalGrates']\n",
    "    for value in list_of_problems:\n",
    "        colum_name = 'has_' + str(value) + '_problem'\n",
    "        df[colum_name] = np.where(df['problems'].str.contains(','.join([value])), 1, 0)              \n",
    "\n",
    "\n",
    "    steward_most_frequent = [x for x in data.steward.value_counts().sort_values(ascending = False).head(3).index]\n",
    "    for value in steward_most_frequent:\n",
    "        colum_name = 'has_' + str(value) + '_steward'\n",
    "        df[colum_name] = np.where(df['steward'] == value, 1, 0)\n",
    "\n",
    "    guards_most_frequent = [x for x in data.guards.value_counts().sort_values(ascending = False).head(3).index]\n",
    "    for value in guards_most_frequent:\n",
    "        colum_name = 'has_' + str(value) + '_guards'\n",
    "        df[colum_name] = np.where(df['guards'] == value, 1, 0)\n",
    "\n",
    "    user_type_most_frequent = [x for x in data.user_type.value_counts().sort_values(ascending = False).head(2).index]\n",
    "    for value in user_type_most_frequent:\n",
    "        colum_name = 'has_' + str(value) + '_user_type'\n",
    "        df[colum_name] = np.where(df['user_type'] == value, 1, 0)\n",
    "\n",
    "    borough_most_frequent = [x for x in data.borough.value_counts().sort_values(ascending = False).head(4).index]\n",
    "    for value in borough_most_frequent:\n",
    "        colum_name = 'has_' + str(value) + '_borough'\n",
    "        df[colum_name] = np.where(df['borough'] == value, 1, 0)\n",
    "\n",
    "    nta_name_most_frequent = [x for x in data.nta_name.value_counts().sort_values(ascending = False).head(5).index]\n",
    "    for value in nta_name_most_frequent:\n",
    "        colum_name = 'has_' + str(value) + '_nta_name'\n",
    "        df[colum_name] = np.where(df['nta_name'] == value, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773acb82-0146-46d5-967a-9f23838741a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Data wrangling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7da9e9-9f80-44a0-b535-5a3213d6a68e",
   "metadata": {},
   "source": [
    "Импорт данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "339a6a4f-c651-40e7-a748-6b17f278958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"C:\\\\Users\\\\skako\\\\Тестовое задание Максибитсолюшен\\\\data\\\\2015-street-tree-census-tree-data.csv\")\n",
    "\n",
    "data, data_validation = train_test_split(data, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9017af07-2208-4fea-aaf8-c62248959c71",
   "metadata": {},
   "source": [
    "Балансировка классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "664690c9-2864-4d74-9a6b-538778db1a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "health\n",
       "Good    24137\n",
       "Fair    24137\n",
       "Poor    24137\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_3, class_2 , class_1 = data.health.value_counts()\n",
    "c3 = data[data['health'] == 'Good']\n",
    "c2 = data[data['health'] == 'Fair']\n",
    "c1 = data[data['health'] == 'Poor']\n",
    "df_3 = c3.sample(class_1)\n",
    "df_2 = c2.sample(class_1)\n",
    "\n",
    "data = pd.concat([df_3, df_2, c1],axis=0)\n",
    "data.dropna(subset = 'health', inplace = True)\n",
    "data['health'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f750577f-8c39-46c4-9ef8-f44d40a2fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace = True, drop = True)\n",
    "data_validation.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3cccf9-95bf-482d-9b18-67b0f9d6b333",
   "metadata": {},
   "source": [
    "Сохранение необработанных выборок в репозиторий. \n",
    "data - сбалансированная выборка для обучение и тестирования моделей.\n",
    "data_validation - часть несбалансированной выборки, котораыя будет использована для итогового тестирования отобранной модели. Это нужно для независимого тетсирования данных без влияния обучающих образцов, а также тестирования модели \"как на проде\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a4a8bd-2c84-4565-8111-b0a22fce5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv((\"C:\\\\Users\\\\skako\\\\Тестовое задание Максибитсолюшен\\\\data\\\\data_balanced_raw.csv\"), index = False)\n",
    "data_validation.to_csv((\"C:\\\\Users\\\\skako\\\\Тестовое задание Максибитсолюшен\\\\data\\\\data_validation.csv\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffef207-ccb6-4d44-a20a-03f3a4e219fb",
   "metadata": {},
   "source": [
    "Отбираем необходимые поля:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e4942a9-897f-4ced-9b74-b004fda57476",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['tree_dbh', 'curb_loc', 'health', 'spc_common', 'steward',\n",
    "                   'guards', 'sidewalk', 'user_type', 'problems', 'root_stone',\n",
    "                   'root_grate', 'root_other', 'trunk_wire', 'trnk_light', 'trnk_other',\n",
    "                   'brch_light', 'brch_shoe', 'brch_other', 'address', \n",
    "                   'zip_city', 'borough', 'nta_name', 'latitude', 'longitude']\n",
    "\n",
    "data = data[columns_to_keep].copy()\n",
    "data_validation = data_validation[columns_to_keep].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fe513a-0757-4235-86a5-1f3c64f9e778",
   "metadata": {},
   "source": [
    "Заполняем пропуски для данных полей значением \"None\", поскольку библиотекой Pandas они ошибочно распознаются как пропуски:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b43e6678-38d5-4782-93cd-f6fe108031eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna({'steward':'None', 'guards':'None', 'problems':'None'}, inplace = True)\n",
    "data_validation.fillna({'steward':'None', 'guards':'None', 'problems':'None'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb350d1-c351-4e26-b7eb-61718c8da7f8",
   "metadata": {},
   "source": [
    "Очистка малозначимых пропусков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88e5b417-b109-4a14-8468-0a14ae9e8a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.dropna(subset = 'spc_common', inplace = True)\n",
    "data_validation.dropna(subset = 'spc_common', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5a7edd-cdba-41f3-a476-92874e79506e",
   "metadata": {},
   "source": [
    "Подготовка данных для формирования основных полей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71839372-3ddb-49a2-aed8-76e8d6e72548",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_poor_health = data[data['health'] == 'Poor'].reset_index()[['latitude', 'longitude', 'health']]\n",
    "lat_lon_poor_health.rename(columns={'latitude': 'latitude_x', 'longitude': 'longitude_x'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b38d9209-ff3b-4fe4-a1b0-897360d7af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wrangling(data, lat_lon_poor_health)\n",
    "data_wrangling(data_validation, lat_lon_poor_health)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080fc996-6b37-42f6-af2d-77fd4459f332",
   "metadata": {},
   "source": [
    "Проведение связки \"Bag of Words + PCA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39e360ac-89a1-4217-85b1-a9f410af7b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_pattern = '(?u)[a-zA-Z][a-z ]+' # so that vectorizer don't split on spaces\n",
    "\n",
    "vec_spc_common, pca_spc_common, scaler_spc_common, data_transformed_normed_spc_common = vectorize_and_pca(data, 'spc_common', (1, 1), token_pattern, 10)\n",
    "vec_problems, pca_problems, scaler_problems, data_transformed_normed_problems = vectorize_and_pca(data, 'problems', (1, 1), token_pattern, 5)\n",
    "vec_address, pca_address, scaler_address, data_transformed_normed_address = vectorize_and_pca(data, 'address', (2, 2), token_pattern, 5)\n",
    "vec_zip_city, pca_zip_city, scaler_zip_city, data_transformed_normed_zip_city = vectorize_and_pca(data, 'zip_city', (1, 1), token_pattern, 5)\n",
    "vec_nta_name, pca_nta_name, scaler_nta_name, data_transformed_normed_nta_name = vectorize_and_pca(data, 'nta_name', (2, 2), token_pattern, 5)\n",
    "\n",
    "data = pd.merge(data, data_transformed_normed_spc_common, left_index=True, right_index=True)\n",
    "data = pd.merge(data, data_transformed_normed_problems, left_index=True, right_index=True)\n",
    "data = pd.merge(data, data_transformed_normed_address, left_index=True, right_index=True)\n",
    "data = pd.merge(data, data_transformed_normed_zip_city, left_index=True, right_index=True)\n",
    "data = pd.merge(data, data_transformed_normed_nta_name, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e9713-3202-4d34-bb1f-fbd1cbd14c35",
   "metadata": {},
   "source": [
    "Скоринг валидационной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5231e85-f2ff-4eed-8b61-6f7e65400b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = [vec_spc_common, vec_problems, vec_address, vec_zip_city, vec_nta_name]\n",
    "pcas = [pca_spc_common, pca_problems, pca_address, pca_zip_city, pca_nta_name]\n",
    "scalers = [scaler_spc_common, scaler_problems, scaler_address, scaler_zip_city, scaler_nta_name]\n",
    "cols = ['spc_common', 'problems', 'address', 'zip_city', 'nta_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b153482b-a71e-4ad4-9ed3-ad789c742ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vecs)):\n",
    "    col_names = [cols[i] + '_pca_component_' + str(j) for j in range(1, pcas[i].n_components_ + 1)]\n",
    "    data_transformed = pcas[i].transform(vecs[i].transform(data_validation[cols[i]]))\n",
    "    data_transformed = pd.DataFrame(data_transformed, columns = col_names)\n",
    "    \n",
    "    scaler = scalers[i]\n",
    "    data_transformed_normed = pd.DataFrame(scaler.transform(data_transformed.values), columns = data_transformed.columns, index = data_transformed.index)\n",
    "    data_validation = pd.merge(data_validation, data_transformed_normed, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5a2a91-48d8-4cbd-97cb-28a4b27cbd01",
   "metadata": {},
   "source": [
    "\"one-hot encoding\" бинарных переменных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88f4f79a-6fe3-4792-b461-3328a82a0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_binary = ['curb_loc', 'sidewalk', \n",
    "                      'root_stone', 'root_grate', 'root_other', \n",
    "                      'trunk_wire', 'trnk_light', 'trnk_other', \n",
    "                      'brch_light', 'brch_shoe', 'brch_other', \n",
    "                      ]\n",
    "data_dummies = pd.get_dummies(data[list_of_binary], dtype = int, drop_first = True)\n",
    "data = pd.concat([data, data_dummies], axis = 1)\n",
    "\n",
    "data_dummies = pd.get_dummies(data_validation[list_of_binary], dtype = int, drop_first = True)\n",
    "data_validation = pd.concat([data_validation, data_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "674ed223-9f7c-407d-8766-8a15e7e77797",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns = {\"has_Annadale-Huguenot-Prince's Bay-Eltingville_nta_name\": \"has_Annadale-Huguenot-Prince Bay-Eltingville_nta_name\"})\n",
    "data_validation = data_validation.rename(columns = {\"has_Annadale-Huguenot-Prince's Bay-Eltingville_nta_name\": \"has_Annadale-Huguenot-Prince Bay-Eltingville_nta_name\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ab262-5dee-453a-9285-5b0c7d3fde43",
   "metadata": {},
   "source": [
    "Отбираем релевантные поля и сохраняем подготовленные выборки в репозиторий:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95fbf541-446e-44d4-8599-1725ba2c9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_select = ['haversine_manhattan_distance_to_poor_tree',\n",
    "                  'tree_dbh_wrangled', 'problems_count', 'has_problem',\n",
    "                  'has_BranchLights_problem',\n",
    "                  'has_TrunkOther_problem', 'has_BranchOther_problem',\n",
    "                  'has_RootOther_problem', 'has_WiresRope_problem',\n",
    "                  'has_MetalGrates_problem', 'has_1or2_steward',\n",
    "                  'has_3or4_steward', 'has_Helpful_guards',\n",
    "                  'has_Harmful_guards', 'has_TreesCount Staff_user_type',\n",
    "                  'has_Queens_borough', 'has_Brooklyn_borough',\n",
    "                  'has_Staten Island_borough', 'has_Bronx_borough',\n",
    "                  'has_Annadale-Huguenot-Prince Bay-Eltingville_nta_name',\n",
    "                  'has_Great Kills_nta_name', 'has_East New York_nta_name',\n",
    "                  'has_Ridgewood_nta_name', 'has_Bayside-Bayside Hills_nta_name',\n",
    "                  'curb_loc_OnCurb', 'sidewalk_NoDamage', 'trnk_light_Yes', 'brch_shoe_Yes',\n",
    "                  'spc_common_pca_component_1', 'spc_common_pca_component_2',\n",
    "                  'spc_common_pca_component_3', 'spc_common_pca_component_4',\n",
    "                  'spc_common_pca_component_5', 'spc_common_pca_component_6',\n",
    "                  'spc_common_pca_component_7', 'spc_common_pca_component_8',\n",
    "                  'spc_common_pca_component_9', 'spc_common_pca_component_10',\n",
    "                  'problems_pca_component_2', 'problems_pca_component_3', \n",
    "                  'problems_pca_component_4', 'problems_pca_component_5', \n",
    "                  'address_pca_component_1',\n",
    "                  'address_pca_component_2', 'address_pca_component_3',\n",
    "                  'address_pca_component_4', 'address_pca_component_5',\n",
    "                  'zip_city_pca_component_1', 'zip_city_pca_component_2',\n",
    "                  'zip_city_pca_component_3', 'zip_city_pca_component_4',\n",
    "                  'zip_city_pca_component_5', \n",
    "                  'nta_name_pca_component_1',\n",
    "                  'nta_name_pca_component_2', 'nta_name_pca_component_3',\n",
    "                  'nta_name_pca_component_4', 'nta_name_pca_component_5', \n",
    "                  'health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a6150db-d4a9-47e4-8cac-b41e8e6d8c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[list_to_select].to_csv((\"\\\\data\\\\data_balanced_wrangled.csv\"), index = False)\n",
    "data_validation[list_to_select].to_csv((\"data\\\\data_validation_wrangled.csv\"), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
